# def step(self, state, action, reward, next_state, done, agent_number):
    #     """Save experience in replay memory, and use random sample from buffer to learn."""
    #     # Save experience / reward
    #     self.memory.add(state, action, reward, next_state, done)

    #     # Learn, if enough samples are available in memory
    #     if len(self.memory) > self.batch_size:
    #         experiences = self.memory.sample()
    #         self.learn(experiences, self.gamma, agent_number)

    # def learn(self, experiences, gamma, agent_number):
    #     """Update policy and value parameters using given batch of experience tuples.
    #     Q_targets = r + Î³ * critic_target(next_state, actor_target(next_state))
    #     where:
    #         actor_target(state) -> action
    #         critic_target(state, action) -> Q-value
    #     Params
    #     ======
    #         experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples 
    #         gamma (float): discount factor
    #     """
    #     states, actions, rewards, next_states, dones = experiences

    #     # ---------------------------- update critic ---------------------------- #
    #     # Get predicted next-state actions and Q values from target models
    #     actions_next = self.actor_target(next_states)
    #     if agent_number == 0:
    #         actions_next = torch.cat((actions_next, actions[:, 2:]), dim=1)
    #     else:
    #         actions_next = torch.cat((actions[:, :2], actions_next), dim=1)
    #     Q_targets_next = self.critic_target(next_states, actions_next)
    #     # Compute Q targets for current states (y_i)
    #     Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))
    #     # Compute critic loss
    #     Q_expected = self.critic_local(states, actions)
    #     critic_loss = F.mse_loss(Q_expected, Q_targets)
    #     # Minimize the loss
    #     self.critic_optimizer.zero_grad()
    #     critic_loss.backward()
    #     self.critic_optimizer.step()

    #     # ---------------------------- update actor ---------------------------- #
    #     # Compute actor loss
    #     actions_pred = self.actor_local(states)
    #     if agent_number == 0:
    #         actions_pred = torch.cat((actions_pred, actions[:, 2:]), dim=1)
    #     else:
    #         actions_pred = torch.cat((actions[:, :2], actions_pred), dim=1)
    #     actor_loss = -self.critic_local(states, actions_pred).mean()
    #     # Minimize the loss
    #     self.actor_optimizer.zero_grad()
    #     actor_loss.backward()
    #     self.actor_optimizer.step()

    #     # ----------------------- update target networks ----------------------- #
    #     self.soft_update(self.critic_local, self.critic_target, self.tau)
    #     self.soft_update(self.actor_local, self.actor_target, self.tau)



    # check action size mismatch while creating critic model and predicting 